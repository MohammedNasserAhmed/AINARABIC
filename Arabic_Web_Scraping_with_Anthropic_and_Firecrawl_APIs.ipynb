{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBT08j2B2Tc9INFfLyFam4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammedNasserAhmed/AINARABIC/blob/main/Arabic_Web_Scraping_with_Anthropic_and_Firecrawl_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Arabic-Web Scraping with Anthropic and Firecrawl APIs ðŸ”ƒ**\n",
        "\n",
        "This notebook demonstrates how to scrape and process data from a specified webpage using the Anthropic and Firecrawl APIs. The example URL used is an Al Jazeera blog post.\n",
        "\n",
        "## *Prerequisites*\n",
        "\n",
        "Ensure you have the following installed:\n",
        "\n",
        "- [Anthropic API](https://www.anthropic.com) (API key required)\n",
        "- [Firecrawl API](https://www.firecrawl.com) (API key required)\n",
        "- Required Python libraries: `requests`, `BeautifulSoup`, `dotenv`, `json`, `anthropic`, `firecrawl`, `textwrap`, `IPython`\n",
        "\n",
        "You can install the necessary libraries using:"
      ],
      "metadata": {
        "id": "V-28c212K4Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install firecrawl anthropic beautifulsoup4 python-dotenv"
      ],
      "metadata": {
        "id": "ujzWUKggn5sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*........ restart session .......*"
      ],
      "metadata": {
        "id": "9Y0x9RFYOVqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from firecrawl import FirecrawlApp\n",
        "import json\n",
        "import anthropic\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from typing import Optional, List\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import Markdown\n",
        "import sys\n",
        "sys.stdout.encoding = 'utf-8'\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "dwhF7PX-PwEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Methods Pool**"
      ],
      "metadata": {
        "id": "CAnPdqInOgdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_main_content_from_html(html: str) -> str:\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        # Find the main content container\n",
        "        main_content = soup.select_one('main#main-content-area')\n",
        "\n",
        "        if main_content:\n",
        "            # Remove unwanted elements (adjust as needed)\n",
        "            for unwanted in main_content.select('.article-info-block,.disclaimer-text, .article-author, .article-dates'):\n",
        "                unwanted.decompose()\n",
        "\n",
        "            # Extract text from all elements that might contain content\n",
        "            content = []\n",
        "            for element in main_content.descendants:\n",
        "                if element.name == 'p':\n",
        "                    content.append(element.get_text().strip())\n",
        "\n",
        "            # Join all content and clean up\n",
        "            full_content = '\\n'.join(content)\n",
        "            full_content = re.sub(r'\\s+', ' ', full_content)  # Replace multiple spaces with single space\n",
        "            full_content = re.sub(r'\\n+', '\\n', full_content)  # Replace multiple newlines with single newline\n",
        "\n",
        "            return full_content.strip()\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "def extract_author_from_html(html: str) -> Optional[str]:\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        author_element = soup.select_one('.article-author__name .author-link')\n",
        "        if author_element:\n",
        "            return author_element.text.strip()\n",
        "        return None\n",
        "\n",
        "def extract_date_from_html(html: str) -> Optional[datetime.date]:\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    date_element = soup.select_one('.article-dates__published')\n",
        "    if date_element:\n",
        "        date_str = date_element.text.strip()\n",
        "        try:\n",
        "            # Assuming the date format is always DD/MM/YYYY\n",
        "            return datetime.strptime(date_str, '%d/%m/%Y').date()\n",
        "        except ValueError:\n",
        "            print(f\"Error parsing date: {date_str}\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def save(data):\n",
        "    with open('output.md', 'w', encoding='utf-8') as file:\n",
        "        file.write(data)\n",
        "\n",
        "    # Save markdown as .json file\n",
        "    with open('output.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "def format_data_dict_to_markdown(data_dict):\n",
        "  markdown_text = \"\"\n",
        "  for key, value in data_dict.items():\n",
        "    if value:\n",
        "      markdown_text += f\"**{key}:** {value}\\n\"\n",
        "  return markdown_text\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "def extract_author(markdown: str):\n",
        "    # Pattern to match Arabic names, potentially preceded by titles like \"Ø¯.\"\n",
        "    author_pattern = r'\\*\\s*(Ø¯\\.\\s*)?[\\u0600-\\u06FF\\s]+\\n'\n",
        "    author_match = re.search(author_pattern, markdown)\n",
        "    if author_match:\n",
        "        author = author_match.group().strip('* \\n')\n",
        "        return author.strip()\n",
        "    return None\n",
        "\n",
        "def extract_date(text):\n",
        "    # This is a simple date extraction. You might need to adjust it based on the actual date format in your content.\n",
        "    date_match = re.search(r'\\d{4}-\\d{2}-\\d{2}', text)\n",
        "    if date_match:\n",
        "        return datetime.strptime(date_match.group(), '%Y-%m-%d').date()\n",
        "    return None"
      ],
      "metadata": {
        "id": "DYk2Fn_dOBPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### *Key Functions*\n",
        "\n",
        "- **`extract_main_content_from_html(html)`**: Extracts and cleans the main content from the HTML of the webpage.\n",
        "- **`extract_date_from_html(html)`**: Extracts the publication date of the article.\n",
        "- **`extract_author_from_html(html)`**: Extracts the authorâ€™s name from the article.\n",
        "- **`format_data_dict_to_markdown(data_dict)`**: Formats the extracted data into markdown.\n",
        "- **`save(data)`**: Saves the formatted data into a file.\n"
      ],
      "metadata": {
        "id": "SzL5MILuOESG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Setup*\n",
        "\n",
        "1. **API Keys**: Obtain your API keys from [Anthropic](https://www.anthropic.com) and [Firecrawl](https://www.firecrawl.com).\n",
        "\n",
        "2. **Environment Variables**: Create a `.env` file in your project directory and add your API keys:"
      ],
      "metadata": {
        "id": "DOantuWVLcjd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSMgwoRWnofY"
      },
      "outputs": [],
      "source": [
        "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\") or \"\"\n",
        "firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\") or \"\"\n",
        "#OR#\n",
        "#os.environ[\"ANTHROPIC_API_KEY\"] = '<anthropic_api_key>'\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = 'fc-291e4604a2bb4f4a84e533e6e9e03a70#'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Code Overview**: The code loads the API keys, initializes the clients, and scrapes the content from the specified URL. The data extracted includes the title, description, main content, author, and other metadata, which is then formatted and saved.\n",
        "\n",
        "## Example Usage\n",
        "\n",
        "To scrape data from the provided Al Jazeera blog post:"
      ],
      "metadata": {
        "id": "M-17nBg2L4fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url=u\"https://www.aljazeera.net/opinions/2024/3/9/%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1-%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A-%D8%A3%D8%A8%D8%B9%D8%AF-%D9%85%D9%86-%D9%83%D9%88%D9%86%D9%87-%D9%85%D8%AC%D8%B1%D8%AF\""
      ],
      "metadata": {
        "id": "VbOUW4VmsEXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–¶ execute the following:"
      ],
      "metadata": {
        "id": "PrMLt2DUL-1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client=anthropic.Client(api_key=anthropic_api_key)\n",
        "app = FirecrawlApp(api_key=firecrawl_api_key)\n",
        "scrape_result = app.scrape_url(url, params={'formats': ['markdown', 'html']})"
      ],
      "metadata": {
        "id": "KM7mAWCCswi_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if scrape_result:\n",
        "    print('Collecting data from crawl results:\\n')\n",
        "    html = scrape_result.get('html')\n",
        "    markdown = scrape_result.get('markdown')\n",
        "    metadata = scrape_result.get('metadata')\n",
        "    srcurl = metadata.get('ogUrl')\n",
        "    title = metadata.get('ogTitle')\n",
        "    description = metadata.get('ogDescription')\n",
        "    site=metadata.get('ogSiteName')\n",
        "    clean_content = extract_main_content_from_html(html)\n",
        "    date = extract_date_from_html(html)\n",
        "    author = extract_author_from_html(html)\n",
        "    data_dict = {\n",
        "            'SourceURL': srcurl,\n",
        "            'Title': title,\n",
        "            'Description': description,\n",
        "            'Date': str(date),\n",
        "            'Author': author,\n",
        "            'Site': site,\n",
        "            'MainContent': clean_content\n",
        "        }\n",
        "    data = format_data_dict_to_markdown(data_dict)\n",
        "    save(data)\n",
        "    print(f'Source URL: {data_dict[\"SourceURL\"]}')\n",
        "    print(f'Title: {data_dict[\"Title\"]}')\n",
        "    print(f'Description: {data_dict[\"Description\"]}')\n",
        "    print(f'Date: {data_dict[\"Date\"]}')\n",
        "    print(f'Author: {data_dict[\"Author\"]}')\n",
        "    print(f'Site: {data_dict[\"Site\"]}')\n",
        "    print('Main Content: \\n')\n",
        "to_markdown(clean_content)"
      ],
      "metadata": {
        "id": "b6k5o4gQMewO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(scrape_result.get('markdown'))"
      ],
      "metadata": {
        "id": "GQauG2wM_A34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Additional Notes*\n",
        "\n",
        "- **Error Handling**: Ensure proper error handling for network requests and data extraction.\n",
        "- **Ethical Considerations**: Always respect the terms of service of the websites you scrape and seek permission where necessary.\n"
      ],
      "metadata": {
        "id": "x5u9WdS8NEsr"
      }
    }
  ]
}